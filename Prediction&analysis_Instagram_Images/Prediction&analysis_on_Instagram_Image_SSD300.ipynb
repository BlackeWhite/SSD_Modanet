{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prediction&analysis on Instagram Image SSD300 ",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNalKnnXyWM6yAbTIdEJTIq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BlackeWhite/SSD_Modanet/blob/master/Prediction%26analysis_Instagram_Images/Prediction%26analysis_on_Instagram_Image_SSD300.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r9CkElW_CQ3",
        "colab_type": "text"
      },
      "source": [
        "# Import Library "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPSLo-KukZI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#per salvare file di gorsse dimensioni sul drive \n",
        "!pip install httplib2==0.15.0\n",
        "!pip install google-api-python-client==1.6\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JK3lPvel8mT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install keras==2.2.3\n",
        "!pip install tensorflow-gpu==1.15\n",
        "import os.path\n",
        "if not(os.path.isdir('keras_layers')):\n",
        "  !git clone https://github.com/pierluigiferrari/ssd_keras.git\n",
        "  !mv ./ssd_keras/* . \n",
        "  !rm -r ssd_keras\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "from keras.optimizers import SGD\n",
        "from imageio import imread\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from models.keras_ssd300 import ssd_300\n",
        "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
        "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
        "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
        "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
        "from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
        "\n",
        "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
        "\n",
        "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
        "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
        "from data_generator.object_detection_2d_geometric_ops import Resize\n",
        "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
        "\n",
        "%matplotlib\n",
        "plt.ioff()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPSbxqtc_Txz",
        "colab_type": "text"
      },
      "source": [
        "# Import Instagram Image and weight of SSD300"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOCWGkLV1qRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#Immagini Instagram estratte dagli hastag\n",
        "Test = drive.CreateFile({'id': '1lloX-DOgQ7bG7iT0a0HMpfMNhOa-UvYi'}) \n",
        "Test.GetContentFile(Test['title'])  # Save Drive file as a local file\n",
        "print('Uploaded {}, id {}'.format(Test['title'], Test['id']))\n",
        "!unzip instagram_image.zip\n",
        "#pesi SSD300 with Data augmentation\n",
        "Weight = drive.CreateFile({'id': '1Ldszh6MHnGyQRrmZGn4-gln1SgQPGZ1v'}) \n",
        "Weight.GetContentFile(Weight['title'])  # Save Drive file as a local file\n",
        "print('Uploaded {}, id {}'.format(Weight['title'], Weight['id']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7s-D3bY_0Z2",
        "colab_type": "text"
      },
      "source": [
        "# Load the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KojmxvUmSYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the image size.\n",
        "img_height = 300\n",
        "img_width = 300\n",
        "\n",
        "# 1: Build the Keras model\n",
        "\n",
        "K.clear_session() # Clear previous models from memory.\n",
        "\n",
        "model = ssd_300(image_size=(img_height, img_width, 3),\n",
        "                n_classes=13,\n",
        "                mode='inference',\n",
        "                l2_regularization=0.0005,\n",
        "                scales=[0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05], # The scales for MS COCO are [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05]\n",
        "                aspect_ratios_per_layer=[[1.0, 2.0, 0.5],\n",
        "                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                                         [1.0, 2.0, 0.5],\n",
        "                                         [1.0, 2.0, 0.5]],\n",
        "                two_boxes_for_ar1=True,\n",
        "                steps=[8, 16, 32, 64, 100, 300],\n",
        "                offsets=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n",
        "                clip_boxes=False,\n",
        "                variances=[0.1, 0.1, 0.2, 0.2],\n",
        "                normalize_coords=True,\n",
        "                subtract_mean=[123, 117, 104],\n",
        "                swap_channels=[2, 1, 0],\n",
        "                confidence_thresh=0.5,\n",
        "                iou_threshold=0.45,\n",
        "                top_k=200,\n",
        "                nms_max_output_size=400)\n",
        "\n",
        "# 2: Load the trained weights into the model.\n",
        "\n",
        "# TODO: Set the path of the trained weights.\n",
        "weights_path = './ssd300_Modanet_data-aug_epoch-150_loss-5.3164_val_loss-5.1061.h5'\n",
        "\n",
        "model.load_weights(weights_path, by_name=True)\n",
        "\n",
        "# 3: Compile the model so that Keras won't complain the next time you load it.\n",
        "\n",
        "sgd = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False)\n",
        "\n",
        "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
        "\n",
        "model.compile(optimizer=sgd, loss=ssd_loss.compute_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QHFy2hgbtv2",
        "colab_type": "text"
      },
      "source": [
        "# Creation of folders where save predicted images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxHQIaceadOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir pred_image\n",
        "!mkdir \"pred_image/#tods\"\n",
        "!mkdir \"pred_image/#fendi\"\n",
        "!mkdir \"pred_image/#dolcegabbana\"\n",
        "!mkdir \"pred_image/#puma\"\n",
        "!mkdir \"pred_image/#valentino\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxYfkfEUBaEw",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Prediction of Bounding box and analysis on Instagram Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugm1jI4Omdna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "classes = ['background',\n",
        "\t\t\t\t\t 'bag', 'belt', 'boots', 'footwear',\n",
        "\t\t\t\t\t 'outer', 'dress', 'sunglasses', 'pants',\n",
        "\t\t\t\t\t 'top', 'shorts', 'skirt', 'headwear',\n",
        "\t\t\t\t\t 'scarf/tie']\n",
        "\n",
        "lista_image = []\n",
        "lista_hastag = []\n",
        "lista_hastag = os.listdir(\"/content/instagram_image/\")  #lista contenente i nomi delle 5 cartelle contenente le immagini dei diversi hastag \n",
        "for hast in lista_hastag:            #viene effettuato per ogni hastag \n",
        "\tlista_image= os.listdir(\"/content/instagram_image/\" + hast)\n",
        "\tlista_bag=0               #contatori abiti  \n",
        "\tlista_belt=0\n",
        "\tlista_boots=0\n",
        "\tlista_footwear=0\n",
        "\tlista_outer=0\n",
        "\tlista_dress=0\n",
        "\tlista_sunglasses=0\n",
        "\tlista_pants=0\n",
        "\tlista_top=0\n",
        "\tlista_shorts=0\n",
        "\tlista_skirt=0\n",
        "\tlista_headwear=0\n",
        "\tlista_scarfetie=0\n",
        "\ti=0 # conatore immagini predette \n",
        "\tfor item in lista_image:\n",
        "\t\tf = open(hast + \".csv\", 'a') #file csv per salvare le predizioni delle immagini predette\n",
        "\t\twrt = open(hast + \"_stats.txt\", 'w') \n",
        "\t\torig_images = [] # Store the images here.\n",
        "\t\tinput_images = [] # Store resized versions of the images here.\n",
        "\n",
        "\t\timg_path = '/content/instagram_image/' + hast + \"/\" + item  \n",
        "\n",
        "\t\torig_images.append(imread(img_path))\n",
        "\t\timg = image.load_img(img_path, target_size=(img_height, img_width))\n",
        "\t\timg = image.img_to_array(img) \n",
        "\t\tinput_images.append(img)\n",
        "\t\tinput_images = np.array(input_images)\n",
        "\t\ty_pred = model.predict(input_images)\n",
        "\t\tconfidence_threshold = 0.5\n",
        "\t\ty_pred_thresh = [y_pred[k][y_pred[k,:,1] > confidence_threshold] for k in range(y_pred.shape[0])]\n",
        "\n",
        "\t\tnp.set_printoptions(precision=2, suppress=True, linewidth=90)\n",
        "\t\tprint(\"Predicted boxes for \",item,\"\\n\")\n",
        "\t\tprint('class   conf  xmin   ymin   xmax   ymax')\n",
        "\t\tprint(y_pred_thresh[0])\n",
        "\t\t#stabiliamo una trashold basata sul numero di boundind box per ogni immagine predetta\n",
        "\t  #se il numero di boundind box Ã¨ maggiore l'immagine viene processata e salvata \n",
        "\t  #altrimenti viene scartata \n",
        "\n",
        "\t\tif len(y_pred_thresh[0]) > 0:\n",
        "\t\t\ti=i+1\n",
        "\t\t\tf.write(\"Predicted boxes for image(300x300): \" + str(item) + \"\\n\")\n",
        "\t\t\tf.write('class,conf,xmin,ymin,xmax,ymax \\n')\n",
        "\t\t\tfor box in y_pred_thresh[0]:\n",
        "\t\t\t\tif round(box[0])==1:\n",
        "\t\t\t\t\tlista_bag+=1\n",
        "\t\t\t\tif round(box[0])==2:\n",
        "\t\t\t\t\tlista_belt+=1\n",
        "\t\t\t\telif round(box[0])==3:\n",
        "\t\t\t\t\tlista_boots+=1\n",
        "\t\t\t\telif round(box[0])==4:\n",
        "\t\t\t\t\tlista_footwear+=1 \n",
        "\t\t\t\telif round(box[0])==5:\n",
        "\t\t\t\t\tlista_outer+=1                                                                            \n",
        "\t\t\t\telif round(box[0])==6:\n",
        "\t\t\t\t\tlista_dress+=1\n",
        "\t\t\t\telif round(box[0])==7:\n",
        "\t\t\t\t\tlista_sunglasses+=1   \n",
        "\t\t\t\telif round(box[0])==8:\n",
        "\t\t\t\t\tlista_pants+=1  \n",
        "\t\t\t\telif round(box[0])==9:\n",
        "\t\t\t\t\tlista_top+=1                                                          \n",
        "\t\t\t\telif round(box[0])==10:\n",
        "\t\t\t\t\tlista_shorts+=1\n",
        "\t\t\t\telif round(box[0])==11:\n",
        "\t\t\t\t\tlista_skirt+=1        \n",
        "\t\t\t\telif round(box[0])==12:\n",
        "\t\t\t\t\tlista_headwear+=1   \n",
        "\t\t\t\telif round(box[0])==13:\n",
        "\t\t\t\t\tlista_scarfetie+=1   \n",
        "\t\t\t\tf.write(str(box[0]) + \",\" + str(round(box[1],2)) + \",\" + str(round(box[2],2)) + \",\" + str(round(box[3],2)) + \",\" + str(round(box[4],2)) + \",\" + str(round(box[5],2)) +\"\\n\" )\n",
        "\t\t\tcolors = plt.cm.hsv(np.linspace(0, 1, 21)).tolist()\n",
        "\t\t\tplt.figure(figsize=(20,12))\n",
        "\t\t\tplt.axis('off')\n",
        "\t\t\tplt.imshow(orig_images[0])\n",
        "\t\t\tcurrent_axis = plt.gca()\n",
        "\t\t\tf.write(\"\\nPredicted boxes for original iamge: \" + str(item) + \"\\n\")\n",
        "\t\t\tf.write('class,conf,xmin,ymin,xmax,ymax \\n')\n",
        "\t\t\tfor box in y_pred_thresh[0]:\n",
        "\t\t\t\t# Transform the predicted bounding boxes for the 300x300 image to the original image dimensions.\n",
        "\t\t\t\txmin = box[2] * orig_images[0].shape[1] / img_width\n",
        "\t\t\t\tymin = box[3] * orig_images[0].shape[0] / img_height\n",
        "\t\t\t\txmax = box[4] * orig_images[0].shape[1] / img_width\n",
        "\t\t\t\tymax = box[5] * orig_images[0].shape[0] / img_height\n",
        "\t\t\t\tf.write(str(box[0]) + \",\" + str(round(box[1],2)) + \",\" + str(round(xmin,2)) + \",\" + str(round(ymin,2)) + \",\" + str(round(xmax,2)) + \",\" + str(round(ymax,2)) +\"\\n\" )\n",
        "\t\t\t\tcolor = colors[int(box[0])]\n",
        "\t\t\t\tlabel = '{}: {:.2f}'.format(classes[int(box[0])], box[1])\n",
        "\t\t\t\tcurrent_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))  \n",
        "\t\t\t\tcurrent_axis.text(xmin, ymin, label, size='x-large', color='black', bbox={'facecolor':color, 'alpha':1.0})  \n",
        "\t\t\t\tplt.savefig('./pred_image/' + hast + \"/\" + item, bbox_inches='tight', pad_inches=0) \n",
        "\t\t\tplt.close()\n",
        "\t\t\tf.write(\"-------------------------------------- \\n\")\n",
        "\t\tdel y_pred_thresh, y_pred\n",
        "\twrt.write(\"N_images: \" + str(i) + \"\\n\")\n",
        "\twrt.write(\"Bag: \" + str(lista_bag)+ \"\\n\")\n",
        "\twrt.write(\"Belt: \" + str(lista_belt)+ \"\\n\")\n",
        "\twrt.write(\"Boots: \" + str(lista_boots)+ \"\\n\")\n",
        "\twrt.write(\"Footwear: \" + str(lista_footwear)+ \"\\n\")\n",
        "\twrt.write(\"Outer: \"+str(lista_outer)+ \"\\n\")\n",
        "\twrt.write(\"Dress: \"+str(lista_dress)+ \"\\n\")\n",
        "\twrt.write(\"Sunglasses: \"+str(lista_sunglasses)+ \"\\n\")\n",
        "\twrt.write(\"Pants: \"+str(lista_pants)+ \"\\n\")\n",
        "\twrt.write(\"Top: \"+str(lista_top)+ \"\\n\")\n",
        "\twrt.write(\"Shorts: \"+str(lista_shorts)+ \"\\n\")\n",
        "\twrt.write(\"Skirt: \"+str(lista_skirt)+ \"\\n\")\n",
        "\twrt.write(\"Headwear: \"+str(lista_headwear)+ \"\\n\")\n",
        "\twrt.write(\"Scarf&tie: \"+ str(lista_scarfetie)+ \"\\n\")  \n",
        "\twrt.close()\n",
        "\tf.close() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vadjDrcrfsHT",
        "colab_type": "text"
      },
      "source": [
        "# Save predict images and annotations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CD88_JhmzgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip results.zip pred_image/*/* \n",
        "!cp results.zip \"/content/drive/My Drive/SSD300_maggiore_di_1(almeno due bounding box)\"\n",
        "!cp \"#dolcegabbana_stats.txt\" \"/content/drive/My Drive/SSD300_maggiore_di_1(almeno due bounding box)\"\n",
        "!cp \"#valentino_stats.txt\" \"/content/drive/My Drive/SSD300_maggiore_di_1(almeno due bounding box)\"\n",
        "!cp \"#fendi_stats.txt\" \"/content/drive/My Drive/SSD300_maggiore_di_1(almeno due bounding box)\"\n",
        "!cp \"#puma_stats.txt\" \"/content/drive/My Drive/SSD300_maggiore_di_1(almeno due bounding box)\"\n",
        "!cp \"#tods_stats.txt\" \"/content/drive/My Drive/SSD300_maggiore_di_1(almeno due bounding box)\"\n",
        "!cp \"#dolcegabbana.csv\" \"/content/drive/My Drive/SSD300_maggiore_di_1(almeno due bounding box)\"\n",
        "!cp \"#valentino.csv\" \"/content/drive/My Drive/SSD300_maggiore_di_1(almeno due bounding box)\"\n",
        "!cp \"#fendi.csv\" \"/content/drive/My Drive/SSD300_maggiore_di_1(almeno due bounding box)\"\n",
        "!cp \"#puma.csv\" \"/content/drive/My Drive/SSD300_maggiore_di_1(almeno due bounding box)\"\n",
        "!cp \"#tods.csv\" \"/content/drive/My Drive/SSD300_maggiore_di_1(almeno due bounding box)\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_yWX8QTgPLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip results.zip pred_image/*/* \n",
        "!cp results.zip \"/content/drive/My Drive/SSD300_maggiore_di_0(almeno un bounding box)\"\n",
        "!cp \"#dolcegabbana_stats.txt\" \"/content/drive/My Drive/SSD300_maggiore_di_0(almeno un bounding box)\"\n",
        "!cp \"#valentino_stats.txt\" \"/content/drive/My Drive/SSD300_maggiore_di_0(almeno un bounding box)\"\n",
        "!cp \"#fendi_stats.txt\" \"/content/drive/My Drive/SSD300_maggiore_di_0(almeno un bounding box)\"\n",
        "!cp \"#puma_stats.txt\" \"/content/drive/My Drive/SSD300_maggiore_di_0(almeno un bounding box)\"\n",
        "!cp \"#tods_stats.txt\" \"/content/drive/My Drive/SSD300_maggiore_di_0(almeno un bounding box)\"\n",
        "!cp \"#dolcegabbana.csv\" \"/content/drive/My Drive/SSD300_maggiore_di_0(almeno un bounding box)\"\n",
        "!cp \"#valentino.csv\" \"/content/drive/My Drive/SSD300_maggiore_di_0(almeno un bounding box)\"\n",
        "!cp \"#fendi.csv\" \"/content/drive/My Drive/SSD300_maggiore_di_0(almeno un bounding box)\"\n",
        "!cp \"#puma.csv\" \"/content/drive/My Drive/SSD300_maggiore_di_0(almeno un bounding box)\"\n",
        "!cp \"#tods.csv\" \"/content/drive/My Drive/SSD300_maggiore_di_0(almeno un bounding box)\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}