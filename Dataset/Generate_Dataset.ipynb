{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generate_Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BlackeWhite/SSD_Modanet/blob/master/Generate_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl9vV3M44AjK",
        "colab_type": "text"
      },
      "source": [
        "# Import File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7fQJ4p-3snQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#dataset Modanet originale \n",
        "fileId = drive.CreateFile({'id': '1FclI6xXIUQfg54Se2w22aCnXOfREcA62'}) \n",
        "fileId.GetContentFile(fileId['title'])  # Save Drive file as a local file\n",
        "print('Uploaded {}, id {}'.format(fileId['title'], fileId['id']))\n",
        "!unzip Moda.zip\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST_CgsZY4GAX",
        "colab_type": "text"
      },
      "source": [
        "# Estrazione delle imagini con annotazioni meno presenti nel dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQM6Bz7pXbI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import glob\n",
        "import os\n",
        "\n",
        "base_path = \"./\"\n",
        "path_train = base_path + \"modanet2018_instances_train.json\"\n",
        "path_val = base_path + \"modanet2018_instances_val.json\"\n",
        "\n",
        "def lista_file_directory(directory,Lista_vuota):\n",
        "\tLista_vuota=os.listdir(directory) \n",
        "\ti=0\n",
        "\tfor item in Lista_vuota:\n",
        "\t\ttemp=item.split(\".\")\n",
        "\t\tLista_vuota[i]=temp[0]\n",
        "\t\ti=i+1 \n",
        "\treturn Lista_vuota\n",
        "\n",
        "\n",
        "l_dataset=[]   \n",
        "l_dataset=lista_file_directory(\"./train\",l_dataset)\n",
        "\n",
        "data = {}\n",
        "data[\"annotations\"] = []\n",
        "\n",
        "lista_occhiali=[]\n",
        "lista_stivali=[]\n",
        "lista_cappello=[]\n",
        "lista_pantaloncini=[]\n",
        "lista_sciarpa=[]\n",
        "i=0\n",
        "with open(path_train, \"r\") as f:\n",
        "    lista=json.load(f)\n",
        "\n",
        "ann = lista[\"annotations\"]\n",
        "for image in lista[\"images\"]:\n",
        "    id0=image[\"id\"]\n",
        "    \n",
        "    for a in ann:\n",
        "        if a[\"image_id\"]==id0:\n",
        "            if a[\"category_id\"]==7:\n",
        "                lista_occhiali.append(id0)\n",
        "            elif a[\"category_id\"]==10:\n",
        "                lista_pantaloncini.append(id0)\n",
        "            elif a[\"category_id\"]==3:\n",
        "                lista_stivali.append(id0)\n",
        "            elif a[\"category_id\"]==13:\n",
        "                lista_sciarpa.append(id0)   \n",
        "            elif a[\"category_id\"]==12:\n",
        "                lista_cappello.append(id0)      \n",
        "    i+=1\n",
        "    if i%1000==0:\n",
        "        print(\"ne ho fatti 1000 e sono a \" + str(i))\n",
        "\n",
        "d = open(\"lista_stivali.txt\", \"w\")\n",
        "for temp in lista_stivali:    \n",
        "    item=str(temp) + \".jpg\\n\"\n",
        "    d.write(item)\n",
        "d.close()\n",
        "d = open(\"lista_cappello.txt\", \"w\")\n",
        "for temp in lista_cappello:   \n",
        "    item=str(temp) + \".jpg\\n\"\n",
        "    d.write(item)\n",
        "d.close()   \n",
        "d = open(\"lista_sciarpa.txt\", \"w\")\n",
        "for temp in lista_sciarpa:   \n",
        "    item=str(temp) + \".jpg\\n\"\n",
        "    d.write(item)\n",
        "d.close()   \n",
        "d = open(\"lista_pantaloncini.txt\", \"w\")\n",
        "for temp in lista_pantaloncini: \n",
        "    item=str(temp) + \".jpg\\n\"\n",
        "    d.write(item)\n",
        "d.close()\n",
        "d = open(\"lista_occhiali.txt\", \"w\")\n",
        "for temp in lista_occhiali: \n",
        "    item=str(temp) + \".jpg\\n\"\n",
        "    d.write(item)\n",
        "d.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF706j1pKFss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir sciarpa\n",
        "!mkdir cappello \n",
        "!mkdir pantaloncini \n",
        "!mkdir stivali \n",
        "!mkdir occhiali\n",
        "\n",
        "#creazione delle cartelle dove isnerire le immagini con meno annotazioni\n",
        "import shutil\n",
        "import os\n",
        "tld_list=[]\n",
        "c = open (\"lista_sciarpa.txt\", \"r\")\n",
        "tld_list = [line for line in c]\n",
        "for item in tld_list:\n",
        "\titem=item.strip('\\n')\n",
        "\tif os.path.exists(\"./train/\" + item):\n",
        "\t\tshutil.move(\"./train/\" + item , \"./sciarpa/\" + item) \t\n",
        "tld_list=[]\n",
        "c.close()\n",
        "c = open (\"lista_cappello.txt\", \"r\")\n",
        "tld_list = [line for line in c]\n",
        "for item in tld_list:\n",
        "\titem=item.strip('\\n')\n",
        "\tif os.path.exists(\"./train/\" + item):\n",
        "\t shutil.move(\"./train/\" + item , \"./cappello/\" + item)\n",
        "tld_list=[]\n",
        "c.close()\n",
        "c = open (\"lista_pantaloncini.txt\", \"r\")\n",
        "tld_list = [line for line in c]\n",
        "for item in tld_list:\n",
        "\titem=item.strip('\\n')\n",
        "\tif os.path.exists(\"./train/\" + item):\n",
        "\t\tshutil.move(\"./train/\" + item , \"./pantaloncini/\" + item)\n",
        "tld_list=[]\n",
        "c.close()\n",
        "c = open (\"lista_stivali.txt\", \"r\")\n",
        "tld_list = [line for line in c]\n",
        "for item in tld_list:\n",
        "\titem=item.strip('\\n')\n",
        "\tif os.path.exists(\"./train/\" + item):\n",
        "\t\tshutil.move(\"./train/\" + item , \"./stivali/\" + item)    \n",
        "tld_list=[]\n",
        "c.close()\n",
        "c = open (\"lista_occhiali.txt\", \"r\")\n",
        "tld_list = [line for line in c]\n",
        "for item in tld_list:\n",
        "\titem=item.strip('\\n')\n",
        "\tif os.path.exists(\"./train/\" + item):\n",
        "\t\tshutil.move(\"./train/\" + item , \"./occhiali/\" + item)      \n",
        "c.close() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFdMaqU24lxw",
        "colab_type": "text"
      },
      "source": [
        "# Creazione del Dataset \n",
        "Train 80% - Validation 10% - Test 10%\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM666CSG7iUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir new_train\n",
        "!mkdir new_val \n",
        "!mkdir new_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jve7QWTW4cPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "l_moda_train = []\n",
        "path=\"./stivali/\" #va fatto anche per le altre 5 cartelle create sopra e per i restanti file \n",
        "l_moda=os.listdir(path) \n",
        "x=len(l_moda)\n",
        "train=0.8\n",
        "val=0.9\n",
        "Inizio=0\n",
        "Fine_train=round(x*train)\n",
        "Fine_val=round(x*val)\n",
        "print(x)\n",
        "print(Fine_train)\n",
        "print(Fine_val)\n",
        "\n",
        "for y in range(Inizio,Fine_train):\n",
        "\tshutil.move(path + l_moda[y], \"./new_train/\" + l_moda[y])\n",
        "for y in range(Fine_train+1,Fine_val):\n",
        "\tshutil.move(path + l_moda[y], \"./new_val/\" + l_moda[y])\n",
        "for y in range(Fine_val+1,x):\n",
        "\tshutil.move(path + l_moda[y], \"./new_test/\" + l_moda[y]) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK-r-vKF5W4m",
        "colab_type": "text"
      },
      "source": [
        "# Creazione dei Json contenenti le annotazione per ogni set del Dataset (Train/val/test)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZihtMmGXLzx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "0bd40d00-a1d0-4acb-9710-6fe069c980c3"
      },
      "source": [
        "import json\n",
        "import glob\n",
        "import os\n",
        "from random import randint \n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "path_train = \"./modanet2018_instances_train.json\"\n",
        "\n",
        "\n",
        "\n",
        "def lista_file_directory(directory,Lista_vuota):\n",
        "    Lista_vuota=os.listdir(directory) \n",
        "    i=0\n",
        "    for item in Lista_vuota:\n",
        "        temp=item.split(\".\")\n",
        "        Lista_vuota[i]=temp[0]\n",
        "        i=i+1 \n",
        "    return Lista_vuota\n",
        "\n",
        "#va fatto per ogni set del dataset (train/val/test)\n",
        "\n",
        "l_dataset=[]   \n",
        "l_dataset=lista_file_directory(\"./new_test\",l_dataset) #va fatto per ogni set del dataset (train/val/test)\n",
        "\n",
        "i=0\n",
        "\n",
        "data = {}\n",
        "data[\"images\"] = []\n",
        "data[\"type\"] = []\n",
        "data[\"annotations\"] = []\n",
        "data[\"categories\"] = []\n",
        "\n",
        "\n",
        "with open(path_train, \"r\") as f:\n",
        "    lista=json.load(f)\n",
        "\n",
        "temp_lista=[]\n",
        "for x in range(0,len(lista[\"images\"])):\n",
        "    temp_lista.append(x) \n",
        "i=0\n",
        "prova=True\n",
        "ann = lista[\"annotations\"]\n",
        "ann_id0 = []\n",
        "for image_name in l_dataset:\n",
        "    id0=0\n",
        "    image_id_store=0\n",
        "    for image_id in temp_lista:\n",
        "        if lista[\"images\"][image_id][\"id\"]==int(image_name):\n",
        "            image_id_store=image_id\n",
        "            temp_lista.remove(image_id)\n",
        "            id0 = lista[\"images\"][image_id][\"id\"]\n",
        "            break    \n",
        "    data[\"images\"].append(lista[\"images\"][image_id_store])\n",
        "\n",
        "    if prova==True:\n",
        "        data[\"type\"].append(lista[\"type\"])\n",
        "        prova=False\n",
        "\n",
        "\n",
        "    for a in ann:\n",
        "        if a[\"image_id\"]==id0:\n",
        "            ann_id0.append(a)\n",
        "\n",
        "    i+=1\n",
        "    if i%1000==0:\n",
        "        print(\"ne ho fatti 1000 e sono a \" + str(i))\n",
        "\n",
        "data[\"annotations\"]=ann_id0\n",
        "data[\"categories\"]=lista[\"categories\"]\n",
        "\n",
        "with open('annotazioni_test.json', 'a') as outfile:\n",
        "    json.dump(data, outfile)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ne ho fatti 1000 e sono a 1000\n",
            "ne ho fatti 1000 e sono a 2000\n",
            "ne ho fatti 1000 e sono a 3000\n",
            "ne ho fatti 1000 e sono a 4000\n",
            "ne ho fatti 1000 e sono a 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiHS9RDnoiRu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2303559c-a364-44ed-fd57-b3364d4f828e"
      },
      "source": [
        "!zip annotazioni.zip annotazioni_train.json annotazioni_val.json annotazioni_test.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: annotazioni_train.json (deflated 74%)\n",
            "  adding: annotazioni_val.json (deflated 74%)\n",
            "  adding: annotazioni_test.json (deflated 74%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIAhA2-hE-yf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#dataset\n",
        "Modadat = drive.CreateFile({'id': '1il_eW1COHs1_QijMh3zklvhmE_Dfk0qh'}) \n",
        "Modadat.GetContentFile(Modadat['title'])  # Save Drive file as a local file\n",
        "print('Uploaded {}, id {}'.format(Modadat['title'], Modadat['id']))\n",
        "!unzip dataset.zip \n",
        "#annotazioni \n",
        "annotaz = drive.CreateFile({'id': '1aD4BgDD7grEFYlDKr0k_SHPw1qbt9N1q'}) \n",
        "annotaz.GetContentFile(annotaz['title'])  # Save Drive file as a local file\n",
        "print('Uploaded {}, id {}'.format(annotaz['title'], annotaz['id']))\n",
        "!unzip annotazioni.zip "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tyfp6FIyoVU",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# verifiica di tutte le annotazioni per ogni file json del set (in modo da elimianre i file senza annotazioni)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOkiXk_zFFKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "import json \n",
        "\n",
        "path = \"./annotazioni_train.json\"\n",
        "\n",
        "i=0\n",
        "data = {}\n",
        "data[\"images\"] = []\n",
        "data[\"annotations\"] = []\n",
        "\n",
        "\n",
        "w = open(\"./train.txt\",\"w\")\n",
        "\n",
        "with open(path, \"r\") as f:\n",
        "    lista=json.load(f)\n",
        "    for image_id in range(0,41908):\n",
        "        id0 = lista[\"images\"][image_id][\"id\"]\n",
        "        ann = lista[\"annotations\"]\n",
        "        ann_id0 = []\n",
        "        for a in ann:\n",
        "            if a[\"image_id\"]==id0:\n",
        "                ann_id0.append(a)\n",
        "        if len(ann_id0)==0:      \n",
        "            w.write(\"ID=\" + str(id0) + \" n annotazioni = \" + str(len(ann_id0)) +\"\\n\")\n",
        "        i+=1\n",
        "        if i%1000==0:\n",
        "            print(\"ne ho fatti 1000 e sono a \" + str(i))    \n",
        "w.close()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6DIYg1r90zV",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# conteggio elementi delle varie categorie del dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B3FLtpII9zV3",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import glob\n",
        "import os\n",
        "\n",
        "\n",
        "#path_train = \"/content/annotazioni_test.json\"\n",
        "\n",
        "base_path = \"./\"\n",
        "path_train = base_path + \"modanet2018_instances_train.json\"\n",
        "\n",
        "data = {}\n",
        "data[\"annotations\"] = []\n",
        "\n",
        "lista_bag=0\n",
        "lista_belt=0\n",
        "lista_boots=0\n",
        "lista_footwear=0\n",
        "lista_outer=0\n",
        "lista_dress=0\n",
        "lista_sunglasses=0\n",
        "lista_pants=0\n",
        "lista_top=0\n",
        "lista_shorts=0\n",
        "lista_skirt=0\n",
        "lista_headwear=0\n",
        "lista_scarfetie=0\n",
        "i=0\n",
        "with open(path_train, \"r\") as f:\n",
        "    lista=json.load(f)\n",
        "\n",
        "ann = lista[\"annotations\"]\n",
        "for image in lista[\"images\"]:\n",
        "    id0=image[\"id\"]\n",
        "    \n",
        "    for a in ann:\n",
        "        if a[\"image_id\"]==id0:\n",
        "            if a[\"category_id\"]==1:\n",
        "                lista_bag+=1\n",
        "            if a[\"category_id\"]==2:\n",
        "                lista_belt+=1\n",
        "            elif a[\"category_id\"]==3:\n",
        "                lista_boots+=1\n",
        "            elif a[\"category_id\"]==4:\n",
        "                lista_footwear+=1 \n",
        "            elif a[\"category_id\"]==5:\n",
        "                lista_outer+=1                                                                            \n",
        "            elif a[\"category_id\"]==6:\n",
        "                lista_dress+=1\n",
        "            elif a[\"category_id\"]==7:\n",
        "                lista_sunglasses+=1   \n",
        "            elif a[\"category_id\"]==8:\n",
        "                lista_pants+=1  \n",
        "            elif a[\"category_id\"]==9:\n",
        "                lista_top+=1                                                          \n",
        "            elif a[\"category_id\"]==10:\n",
        "                lista_shorts+=1\n",
        "            elif a[\"category_id\"]==11:\n",
        "                lista_skirt+=1        \n",
        "            elif a[\"category_id\"]==12:\n",
        "                lista_headwear+=1   \n",
        "            elif a[\"category_id\"]==13:\n",
        "                lista_scarfetie+=1     \n",
        "    i+=1\n",
        "    if i%1000==0:\n",
        "        print(\"ne ho fatti 1000 e sono a \" + str(i))\n",
        "\n",
        "print(\"Bag: \" + str(lista_bag))\n",
        "print(\"Belt:\" + str(lista_belt))\n",
        "print(\"Boots: \" + str(lista_boots))\n",
        "print(\"Footwear:\" + str(lista_footwear))\n",
        "print(\"Outer: \"+str(lista_outer))\n",
        "print(\"Dress: \"+str(lista_dress))\n",
        "print(\"Sunglasses: \"+str(lista_sunglasses))\n",
        "print(\"Pants: \"+str(lista_pants))\n",
        "print(\"Top: \"+str(lista_top))\n",
        "print(\"Shorts: \"+str(lista_shorts))\n",
        "print(\"Skirt: \"+str(lista_skirt))\n",
        "print(\"Headwear: \"+str(lista_headwear))\n",
        "print(\"Scarf&tie: \"+ str(lista_scarfetie))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlWgtWWo4v8b",
        "colab_type": "text"
      },
      "source": [
        "# Test Tenosrflow 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW2_zfsb4Xyc",
        "colab_type": "text"
      },
      "source": [
        "# Elenco di tutti i file presnti nel set di train e nel set di val (usati per i test delle tensorflow)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUu4p0eBeSbx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "path1=\"./new_train/\"\n",
        "path2=\"./new_val/\"\n",
        "\n",
        "\n",
        "lista_train=os.listdir(path1)\n",
        "lista_val=os.listdir(path2)\n",
        "\n",
        "\n",
        "t=open(\"train.txt\", 'a')\n",
        "v=open(\"val.txt\",'a')\n",
        "\n",
        "\n",
        "for item in lista_train:\n",
        "  x=item.split(\".\")\n",
        "  t.write(x[0] + \"\\n\")\n",
        "for item in lista_val:\n",
        "  x=item.split(\".\")\n",
        "  v.write(x[0] + \"\\n\")  \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZncOrrnW4shq",
        "colab_type": "text"
      },
      "source": [
        "# Creazione annotazioni in formato xml per ogni file del dataset estrandole dai file json fatti in precedenza"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So7FVt2xoO4v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "f5a715d8-9bb3-41b8-e84f-35e336087fc2"
      },
      "source": [
        "import json\n",
        "import glob\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "#va fatto sia per il set di train che per il val \n",
        "\n",
        "\n",
        "path_train = \"./annotazioni_val.json\"\n",
        "classes = ['background',\n",
        "           'bag', 'belt', 'boots', 'footwear',\n",
        "           'outer', 'dress', 'sunglasses', 'pants',\n",
        "           'top', 'shorts', 'skirt', 'headwear',\n",
        "           'scarf/tie']\n",
        "\n",
        "def lista_file_directory(directory,Lista_vuota):\n",
        "    Lista_vuota=os.listdir(directory) \n",
        "    i=0\n",
        "    for item in Lista_vuota:\n",
        "        temp=item.split(\".\")\n",
        "        Lista_vuota[i]=temp[0]\n",
        "        i=i+1 \n",
        "    return Lista_vuota\n",
        "\n",
        "with open(path_train, \"r\") as f:\n",
        "    lista=json.load(f)\n",
        "   \n",
        "\n",
        "l_dataset=[]   \n",
        "l_dataset=lista_file_directory(\"./new_val\",l_dataset) #va fatto per ogni set del dataset \n",
        "#l_dataset=lista_file_directory(\"./new_train\",l_dataset)\n",
        "i=0\n",
        "\n",
        "\n",
        "temp_lista=[]\n",
        "for x in range(0,len(lista[\"images\"])):\n",
        "    temp_lista.append(x) \n",
        "i=0\n",
        "\n",
        "\n",
        "\n",
        "ann = lista[\"annotations\"]\n",
        "ann_id0 = []\n",
        "for image_name in l_dataset:\n",
        "    root = ET.Element('annotation')\n",
        "    folder = ET.SubElement(root, 'folder')\n",
        "    folder.text = 'new_train'\n",
        "    filename = ET.SubElement(root, 'filename')\n",
        "    filename.text = image_name + \".jpg\"\n",
        "    size = ET.SubElement(root, 'size')\n",
        "    width = ET.SubElement(size, 'width')\n",
        "    width.text = '400'\n",
        "    height = ET.SubElement(size, 'height')\n",
        "    height.text = '600'\n",
        "    segmented = ET.SubElement(root, 'segmented')\n",
        "    segmented.text = '0'\n",
        "\n",
        "\n",
        "    id0=0\n",
        "    image_id_store=0\n",
        "    for image_id in temp_lista:\n",
        "        if lista[\"images\"][image_id][\"id\"]==int(image_name):\n",
        "            image_id_store=image_id\n",
        "            temp_lista.remove(image_id)\n",
        "            id0 = lista[\"images\"][image_id][\"id\"]\n",
        "            break    \n",
        "\n",
        "\n",
        "    for a in ann:\n",
        "        if a[\"image_id\"]==id0:\n",
        "            object_1 = ET.SubElement(root, 'object')\n",
        "            category_name = ET.SubElement(object_1, 'name')\n",
        "            category_name.text = classes[a['category_id']]\n",
        "            bndbox = ET.SubElement(object_1, 'bndbox')\n",
        "            xmin = ET.SubElement(bndbox, 'xmin')\n",
        "            xmin.text = str(a['bbox'][0])\n",
        "            ymin = ET.SubElement(bndbox, 'ymin')\n",
        "            ymin.text = str(a['bbox'][1])\n",
        "            xmax = ET.SubElement(bndbox, 'xmax')\n",
        "            xmax.text = str(a['bbox'][2])\n",
        "            ymax = ET.SubElement(bndbox, 'ymax')\n",
        "            ymax.text = str(a['bbox'][3])\n",
        "                    \n",
        "   \n",
        " \n",
        "    tree = ET.ElementTree(root)\n",
        "    tree.write(\"./annotazioni_val/\"+str(image_name)+\".xml\", pretty_print=True)\n",
        "    i+=1\n",
        "    if i%1000==0:\n",
        "        print(\"ne ho fatti 1000 e sono a \" + str(i))\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ne ho fatti 1000 e sono a 1000\n",
            "ne ho fatti 1000 e sono a 2000\n",
            "ne ho fatti 1000 e sono a 3000\n",
            "ne ho fatti 1000 e sono a 4000\n",
            "ne ho fatti 1000 e sono a 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y85DIcMQ6g7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip annotazioni_xml.zip annotazioni_train/* annotazioni_val/*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tTcF9E1ZZX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://gist.github.com/iKhushPatel/ed1f837656b155d9b94d45b42e00f5e4\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "\n",
        "def xml_to_csv(path):\n",
        "    xml_list = []\n",
        "    for xml_file in glob.glob(path + '/*.xml'):\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "        for member in root.findall('object'):\n",
        "            value = (root.find('filename').text,\n",
        "                     int(root.find('size')[0].text),\n",
        "                     int(root.find('size')[1].text),\n",
        "                     member[0].text,\n",
        "                     int(member[1][0].text),\n",
        "                     int(member[1][1].text),\n",
        "                     int(member[1][2].text),\n",
        "                     int(member[1][3].text)\n",
        "                     )\n",
        "            xml_list.append(value)\n",
        "    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
        "    return xml_df\n",
        "\n",
        "#inserisce in un unico csv tutti i file xml del train (analogo per il validation)\n",
        "image_path = '/content/annotazioni_train'\n",
        "xml_df = xml_to_csv(image_path)\n",
        "xml_df.to_csv('./train_labels.csv', index=None)\n",
        "\n",
        "image_path = '/content/annotazioni_val'\n",
        "xml_df = xml_to_csv(image_path)\n",
        "xml_df.to_csv('./val_labels.csv',index=None)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhfmZyMLtWpg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f2a4fe7e-c4ce-40f4-b685-f8b708ef18ce"
      },
      "source": [
        "#https://gist.github.com/iKhushPatel/5614a36f26cf6459cc49c8248e8b5b48\n",
        "#creazione dei file tfrecords per train e val \n",
        "\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import os\n",
        "import io\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from PIL import Image\n",
        "import sys\n",
        "\n",
        "from models.research.object_detection.utils import dataset_util\n",
        "from collections import namedtuple, OrderedDict\n",
        "\n",
        "\n",
        "# TO-DO replace this with label map\n",
        "def class_text_to_int(row_label):\n",
        "    if row_label == 'bag':\n",
        "        return 1\n",
        "    elif row_label == 'belt':\n",
        "        return 2\n",
        "    elif row_label == 'boots':\n",
        "        return 3\n",
        "    elif row_label == 'footwear':\n",
        "        return 4\n",
        "    elif row_label == 'outer':\n",
        "        return 5\n",
        "    elif row_label == 'dress':\n",
        "        return 6\n",
        "    elif row_label == 'sunglasses':\n",
        "        return 7\n",
        "    elif row_label == 'pants':\n",
        "        return 8\n",
        "    elif row_label == 'top':\n",
        "        return 9\n",
        "    elif row_label == 'shorts':\n",
        "        return 10    \n",
        "    elif row_label == 'skirt':\n",
        "        return 11\n",
        "    elif row_label == 'headwear':\n",
        "        return 12\n",
        "    elif row_label == 'scarf/tie':\n",
        "        return 13                                              \n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def split(df, group):\n",
        "    data = namedtuple('data', ['filename', 'object'])\n",
        "    gb = df.groupby(group)\n",
        "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "\n",
        "def create_tf_example(group, path):\n",
        "    with tf.io.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
        "        encoded_jpg = fid.read()\n",
        "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "    image = Image.open(encoded_jpg_io)\n",
        "    width, height = image.size\n",
        "\n",
        "    filename = group.filename.encode('utf8')\n",
        "    image_format = b'jpg'\n",
        "    xmins = []\n",
        "    xmaxs = []\n",
        "    ymins = []\n",
        "    ymaxs = []\n",
        "    classes_text = []\n",
        "    classes = []\n",
        "\n",
        "    for index, row in group.object.iterrows():\n",
        "        xmins.append(row['xmin'] / width)\n",
        "        xmaxs.append(row['xmax'] / width)\n",
        "        ymins.append(row['ymin'] / height)\n",
        "        ymaxs.append(row['ymax'] / height)\n",
        "        classes_text.append(row['class'].encode('utf8'))\n",
        "        classes.append(class_text_to_int(row['class']))\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "        'image/height': dataset_util.int64_feature(height),\n",
        "        'image/width': dataset_util.int64_feature(width),\n",
        "        'image/filename': dataset_util.bytes_feature(filename),\n",
        "        'image/source_id': dataset_util.bytes_feature(filename),\n",
        "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "        'image/format': dataset_util.bytes_feature(image_format),\n",
        "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "    }))\n",
        "    return tf_example\n",
        "\n",
        "\n",
        "\n",
        "writer = tf.io.TFRecordWriter('./val.record')\n",
        "path = os.path.join('/content/new_val')\n",
        "examples = pd.read_csv('/content/val_labels.csv')\n",
        "grouped = split(examples, 'filename')\n",
        "for group in grouped:\n",
        "  tf_example = create_tf_example(group, path)\n",
        "  writer.write(tf_example.SerializeToString())\n",
        "\n",
        "        \n",
        "\n",
        "writer.close()\n",
        "output_path = './val.record'\n",
        "print('Successfully created the TFRecords: {}'.format(output_path))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully created the TFRecords: ./val.record\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
