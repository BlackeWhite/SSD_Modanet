{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Traning_ssd_300.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BlackeWhite/SSD_Modanet/blob/master/SSD300/Traning_ssd_300.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaQuW8H21jAd",
        "colab_type": "text"
      },
      "source": [
        "# Import Google Drive File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ4K9DH26voA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#per salvare file di gorsse dimensioni sul drive \n",
        "!pip install httplib2==0.15.0\n",
        "!pip install google-api-python-client==1.6\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOCWGkLV1qRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "#pesi VGG_ILSVRC_16_layers_fc_reduced\n",
        "Weight = drive.CreateFile({'id': '1sBmajn6vOE7qJ8GnxUJt4fGPuffVUZox'}) \n",
        "Weight.GetContentFile(Weight['title'])  # Save Drive file as a local file\n",
        "print('Uploaded {}, id {}'.format(Weight['title'], Weight['id']))\n",
        "\n",
        "#dataset\n",
        "Modadat = drive.CreateFile({'id': '1il_eW1COHs1_QijMh3zklvhmE_Dfk0qh'}) \n",
        "Modadat.GetContentFile(Modadat['title'])  # Save Drive file as a local file\n",
        "print('Uploaded {}, id {}'.format(Modadat['title'], Modadat['id']))\n",
        "!unzip dataset.zip \n",
        "#annotazioni \n",
        "annotaz = drive.CreateFile({'id': '1aD4BgDD7grEFYlDKr0k_SHPw1qbt9N1q'}) \n",
        "annotaz.GetContentFile(annotaz['title'])  # Save Drive file as a local file\n",
        "print('Uploaded {}, id {}'.format(annotaz['title'], annotaz['id']))\n",
        "!unzip annotazioni.zip "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um78Ov8IpWLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3WbzmGawz1w",
        "colab_type": "text"
      },
      "source": [
        "# Import library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezuUWsHzxFsL",
        "colab_type": "text"
      },
      "source": [
        "Import library from github of Pireluigi Ferrari that had created the code for implement the SSD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD4rDcaNw3ko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install keras==2.2.3\n",
        "#â™¦%tensorflow_version 1.x\n",
        "!pip install tensorflow-gpu==1.15\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import shutil\n",
        "import os.path\n",
        "if not(os.path.isdir('keras_layers')):\n",
        "  !git clone https://github.com/pierluigiferrari/ssd_keras.git\n",
        "  !mv ./ssd_keras/* . #copy all file in the root directory, you must this because after invocate the lib in the repository \n",
        "  !rm -r ssd_keras\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TerminateOnNaN, CSVLogger\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "import tensorflow as tf\n",
        "from math import ceil\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from models.keras_ssd300 import ssd_300\n",
        "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
        "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
        "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
        "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
        "from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
        "\n",
        "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
        "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
        "\n",
        "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
        "from data_generator.object_detection_2d_geometric_ops import Resize\n",
        "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
        "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation, SSDRandomCrop\n",
        "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
        "\n",
        "import logging\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7OvtX5Kr_Ja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "LOG_FILENAME = 'colab.log'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfRRXUur0ze9",
        "colab_type": "text"
      },
      "source": [
        "# Set the model configuration parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wck2x6q0zgJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.debug('Create model configuration parameter')\n",
        "\n",
        "# If you're resuming a previous training, set `initial_epoch` and `final_epoch` accordingly.\n",
        "initial_epoch   = 0\n",
        "final_epoch     = 120\n",
        "steps_per_epoch = 256\n",
        "\n",
        "batch_size=64\n",
        "img_height = 300 # Height of the model input images\n",
        "img_width = 300 # Width of the model input images\n",
        "img_channels = 3 # Number of color channels of the model input images\n",
        "mean_color = [123, 117, 104] # The per-channel mean of the images in the dataset. Do not change this value if you're using any of the pre-trained weights.\n",
        "swap_channels = [2, 1, 0] # The color channel order in the original SSD is BGR, so we'll have the model reverse the color channel order of the input images.\n",
        "n_classes = 13 # Number of positive classes\n",
        "scales = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # The anchor box scaling factors used in the original SSD300 for the MS COCO datasets\n",
        "\n",
        "aspect_ratios = [[1.0, 2.0, 0.5],\n",
        "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                 [1.0, 2.0, 0.5],\n",
        "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
        "two_boxes_for_ar1 = True\n",
        "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
        "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
        "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
        "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are divided as in the original implementation\n",
        "normalize_coords = True\n",
        "logging.debug('Parameters created')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGvJOZou07KZ",
        "colab_type": "text"
      },
      "source": [
        "# Build the model or load the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5wiqdPY1Lbx",
        "colab_type": "text"
      },
      "source": [
        "Create a new model and load trained VGG-16 weights into it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHe7tqI4BzJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Mean_IOU_tensorflow_2(y_true, y_pred):\n",
        "    nb_classes = K.int_shape(y_pred)[-1]\n",
        "    iou = []\n",
        "    true_pixels = K.argmax(y_true, axis=-1)\n",
        "    pred_pixels = K.argmax(y_pred, axis=-1)\n",
        "    void_labels = K.equal(K.sum(y_true, axis=-1), 0)\n",
        "    for i in range(0, nb_classes): # exclude first label (background) and last label (void)\n",
        "        true_labels = K.equal(true_pixels, i) & ~void_labels\n",
        "        pred_labels = K.equal(pred_pixels, i) & ~void_labels\n",
        "        inter = tf.to_float(true_labels & pred_labels)\n",
        "        union = tf.to_float(true_labels | pred_labels)\n",
        "        legal_batches = K.sum(tf.to_int32(true_labels), axis=1)>0\n",
        "        ious = K.sum(inter, axis=1)/K.sum(union, axis=1)\n",
        "        iou.append(K.mean(ious[legal_batches]))\n",
        "    iou = tf.stack(iou)\n",
        "    legal_labels = ~tf.math.is_nan(iou)\n",
        "    iou = iou[legal_labels]\n",
        "    return -K.mean(iou)\n",
        "\n",
        "def iou_loss(y_true,y_pred):\n",
        "  iou=Mean_IOU_tensorflow_2(y_true,y_pred)\n",
        "  return -iou"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBr3g5MI0xHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "K.clear_session() # Clear previous models from memory.\n",
        "\n",
        "model, predictor_sizes = ssd_300(image_size=(img_height, img_width, img_channels),\n",
        "                n_classes=13,\n",
        "                mode='training',\n",
        "                l2_regularization=0.0005,\n",
        "                scales=scales,\n",
        "                aspect_ratios_per_layer=aspect_ratios,\n",
        "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
        "                steps=steps,\n",
        "                offsets=offsets,\n",
        "                clip_boxes=clip_boxes,\n",
        "                variances=variances,\n",
        "                normalize_coords=normalize_coords,\n",
        "                subtract_mean=mean_color,\n",
        "                swap_channels=swap_channels,\n",
        "                return_predictor_sizes=True)\n",
        "\n",
        "# 2: Load some weights into the model.\n",
        "\n",
        "# TODO: Set the path to the weights you want to load.\n",
        "#weights_path = './ssd300_Modanet_data-aug_epoch-150_loss-5.3164_val_loss-5.1061.h5' #pesi all'epoca 150 \n",
        "weights_path = './VGG_ILSVRC_16_layers_fc_reduced.h5' #pesi VGG\n",
        "\n",
        "\n",
        "model.load_weights(weights_path, by_name=True)\n",
        "\n",
        "# 3: Instantiate an optimizer and the SSD loss function and compile the model.\n",
        "#    If you want to follow the original Caffe implementation, use the preset SGD\n",
        "#    optimizer, otherwise I'd recommend the commented-out Adam optimizer.\n",
        "\n",
        "#adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "sgd = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False)\n",
        "\n",
        "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
        "\n",
        "\n",
        "#model.compile(optimizer=adam, loss=ssd_loss.compute_loss, metrics=[\"acc\"])\n",
        "model.compile(optimizer=sgd, loss=ssd_loss.compute_loss,  metrics=[\"acc\"])\n",
        "logging.debug('Model preparation finisched')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbO3KT6n4vjf",
        "colab_type": "text"
      },
      "source": [
        "# Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWDkZZB9sfkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.debug('Load dataset')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyVXZo4N76xl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rename(path):   #funzione per apportare modifiche ai nomi del dataset \n",
        "\tlis = os.listdir(path)\n",
        "\tfor item in lis:\n",
        "\t\ttemp=item.split(\".\")\n",
        "\t\twhile len(temp[0])<7:\n",
        "\t\t\tnew=str(0)+temp[0]\n",
        "\t\t\ttemp[0]=new\n",
        "\t\t\tif len(temp[0])==7:\n",
        "\t\t\t\tos.rename(path+item,path+temp[0]+\".jpg\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4Q4UJzeCEcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rename('./new_train/')\n",
        "rename('./new_val/')\n",
        "rename('./new_test/')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXD2j0FqgDLc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "72977b69-3f40-4ecd-ef01-ac0b1cbb68c6"
      },
      "source": [
        "# TODO: Set the paths to your dataset here.\n",
        "train_images_path = ['./new_train/']\n",
        "train_labels_path = ['annotazioni_train.json']\n",
        "val_images_path= ['./new_val/']\n",
        "val_labels_path = ['annotazioni_val.json']\n",
        "\n",
        "train_dataset = DataGenerator(load_images_into_memory=False)\n",
        "val_dataset = DataGenerator(load_images_into_memory=False)\n",
        "\n",
        "train_dataset.parse_json(images_dirs=train_images_path,\n",
        "                  annotations_filenames=train_labels_path,\n",
        "                  ground_truth_available= True,\n",
        "                  include_classes = 'all',\n",
        "                  ret=True)\n",
        "\n",
        "val_dataset.parse_json(images_dirs=val_images_path,\n",
        "                  annotations_filenames=val_labels_path,\n",
        "                  ground_truth_available= True,\n",
        "                  include_classes = 'all',\n",
        "                  ret=True)\n",
        "\n",
        "print(\"Number of images in the traning dataset:\", train_dataset.get_dataset_size())\n",
        "print(\"Number of images in the validation dataset:\", val_dataset.get_dataset_size())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing 'annotazioni_train.json': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41810/41810 [00:00<00:00, 44667.47it/s]\n",
            "Processing 'annotazioni_val.json': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5225/5225 [00:00<00:00, 90557.71it/s]\n",
            "Number of images in the traning dataset: 41810\n",
            "Number of images in the validation dataset: 5225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8NsFjIPrXcG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "22c142ef-c375-4bdd-92cf-70dee2987e95"
      },
      "source": [
        "# 3: Set the batch size.\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# For the validation generator:\n",
        "convert_to_3_channels = ConvertTo3Channels()\n",
        "resize = Resize(height=img_height, width=img_width)\n",
        "\n",
        "ssd_data_augmentation = SSDDataAugmentation(img_height=img_height,\n",
        "                                            img_width=img_width,\n",
        "                                            background=mean_color)\n",
        "\n",
        "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
        "                                    img_width=img_width,\n",
        "                                    n_classes=n_classes,\n",
        "                                    predictor_sizes=predictor_sizes,\n",
        "                                    scales=scales,\n",
        "                                    aspect_ratios_per_layer=aspect_ratios,\n",
        "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
        "                                    steps=steps,\n",
        "                                    offsets=offsets,\n",
        "                                    clip_boxes=clip_boxes,\n",
        "                                    variances=variances,\n",
        "                                    matching_type='multi',\n",
        "                                    pos_iou_threshold=0.5,\n",
        "                                    neg_iou_limit=0.5,\n",
        "                                    normalize_coords=normalize_coords)\n",
        "\n",
        "\n",
        "\n",
        "# 6: Create the generator handles that will be passed to Keras' `fit_generator()` function.\n",
        "train_generator = train_dataset.generate(batch_size=batch_size,\n",
        "                                         shuffle=True,\n",
        "                                         transformations=[convert_to_3_channels,resize],\n",
        "                                         label_encoder=ssd_input_encoder,\n",
        "                                         returns={'processed_images',\n",
        "                                                  'encoded_labels'},\n",
        "                                         keep_images_without_gt=False)\n",
        "#rename('./new_train/')\n",
        "val_generator = val_dataset.generate(batch_size=batch_size,\n",
        "                                     shuffle=False,\n",
        "                                     transformations=[convert_to_3_channels,resize],\n",
        "                                     label_encoder=ssd_input_encoder,\n",
        "                                     returns={'processed_images',\n",
        "                                              'encoded_labels'},\n",
        "                                     keep_images_without_gt=False)\n",
        "# Get the number of samples in the training and validations datasets.\n",
        "train_dataset_size = train_dataset.get_dataset_size()\n",
        "val_dataset_size   = val_dataset.get_dataset_size()\n",
        "\n",
        "print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
        "print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of images in the training dataset:\t 41810\n",
            "Number of images in the validation dataset:\t  5225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_R_JrDeIsnYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.debug('Data loaded')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VcOeryG62HU",
        "colab_type": "text"
      },
      "source": [
        "Set the remaining training parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZAHo__U6y2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.debug('Training preparation')\n",
        "# Define a learning rate schedule.\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    if epoch < 80:\n",
        "        return 0.001\n",
        "    elif epoch < 100:\n",
        "        return 0.0001\n",
        "    else:\n",
        "        return 0.00001\n",
        "# Define model callbacks.\n",
        "\n",
        "# TODO: Set the filepath under which you want to save the model.\n",
        "model_checkpoint = ModelCheckpoint(filepath='/content/drive/My Drive/pesi_SSD/ssd300_Modanet_epoch-{epoch:02d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5',\n",
        "                                   monitor='val_loss',\n",
        "                                   verbose=1,\n",
        "                                   save_best_only=True,\n",
        "                                   save_weights_only=False,\n",
        "                                   mode='auto',\n",
        "                                   period=1)\n",
        "#model_checkpoint.best = \n",
        "\n",
        "csv_logger = CSVLogger(filename='/content/drive/My Drive/log_SSD300.csv',\n",
        "                       separator=',',\n",
        "                       append=True)\n",
        "\n",
        "learning_rate_scheduler = LearningRateScheduler(schedule=lr_schedule,\n",
        "                                                verbose=1)\n",
        "\n",
        "terminate_on_nan = TerminateOnNaN()\n",
        "\n",
        "callbacks = [model_checkpoint,\n",
        "             csv_logger,\n",
        "             learning_rate_scheduler,\n",
        "             terminate_on_nan]\n",
        "\n",
        "logging.debug('Training prepared')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzso6gke7CKE",
        "colab_type": "text"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahAHibUG7Eiz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cab0ef56-1067-404b-afa3-054b682e9e15"
      },
      "source": [
        "logging.debug('Training start')\n",
        "\n",
        "\n",
        "\n",
        "history = model.fit_generator(generator=train_generator,\n",
        "                              steps_per_epoch=steps_per_epoch,\n",
        "                              epochs=final_epoch,\n",
        "                              callbacks=callbacks,\n",
        "                              validation_data=val_generator,\n",
        "                              validation_steps=ceil(val_dataset_size/batch_size),\n",
        "                              initial_epoch=initial_epoch)\n",
        "\n",
        "logging.debug('Training end')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/120\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
            "256/256 [==============================] - 448s 2s/step - loss: 12.0437 - acc: 0.3680 - val_loss: 8.7845 - val_acc: 0.3224\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 8.78454, saving model to /content/drive/My Drive/pesi_SSD/ssd300_Modanet_epoch-01_loss-12.0436_val_loss-8.7845.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/utils/io_utils.py:186: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
            "  self.data = h5py.File(path,)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/120\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.001.\n",
            "256/256 [==============================] - 408s 2s/step - loss: 8.3288 - acc: 0.3532 - val_loss: 7.8785 - val_acc: 0.3974\n",
            "\n",
            "Epoch 00002: val_loss improved from 8.78454 to 7.87850, saving model to /content/drive/My Drive/pesi_SSD/ssd300_Modanet_epoch-02_loss-8.3288_val_loss-7.8785.h5\n",
            "Epoch 3/120\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.001.\n",
            "256/256 [==============================] - 411s 2s/step - loss: 7.6731 - acc: 0.3869 - val_loss: 7.5045 - val_acc: 0.4237\n",
            "\n",
            "Epoch 00003: val_loss improved from 7.87850 to 7.50454, saving model to /content/drive/My Drive/pesi_SSD/ssd300_Modanet_epoch-03_loss-7.6732_val_loss-7.5045.h5\n",
            "Epoch 4/120\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.001.\n",
            "256/256 [==============================] - 411s 2s/step - loss: 7.3690 - acc: 0.4105 - val_loss: 7.2916 - val_acc: 0.4060\n",
            "\n",
            "Epoch 00004: val_loss improved from 7.50454 to 7.29158, saving model to /content/drive/My Drive/pesi_SSD/ssd300_Modanet_epoch-04_loss-7.3690_val_loss-7.2916.h5\n",
            "Epoch 5/120\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.001.\n",
            "256/256 [==============================] - 420s 2s/step - loss: 7.2081 - acc: 0.4211 - val_loss: 7.1086 - val_acc: 0.4236\n",
            "\n",
            "Epoch 00005: val_loss improved from 7.29158 to 7.10864, saving model to /content/drive/My Drive/pesi_SSD/ssd300_Modanet_epoch-05_loss-7.2081_val_loss-7.1086.h5\n",
            "Epoch 6/120\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.001.\n",
            "256/256 [==============================] - 406s 2s/step - loss: 7.0261 - acc: 0.4320 - val_loss: 6.9994 - val_acc: 0.4514\n",
            "\n",
            "Epoch 00006: val_loss improved from 7.10864 to 6.99941, saving model to /content/drive/My Drive/pesi_SSD/ssd300_Modanet_epoch-06_loss-7.0246_val_loss-6.9994.h5\n",
            "Epoch 7/120\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.001.\n",
            "256/256 [==============================] - 406s 2s/step - loss: 6.8994 - acc: 0.4458 - val_loss: 6.8901 - val_acc: 0.4546\n",
            "\n",
            "Epoch 00007: val_loss improved from 6.99941 to 6.89009, saving model to /content/drive/My Drive/pesi_SSD/ssd300_Modanet_epoch-07_loss-6.8994_val_loss-6.8901.h5\n",
            "Epoch 8/120\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.001.\n",
            "256/256 [==============================] - 402s 2s/step - loss: 6.7991 - acc: 0.4512 - val_loss: 6.8262 - val_acc: 0.4589\n",
            "\n",
            "Epoch 00008: val_loss improved from 6.89009 to 6.82616, saving model to /content/drive/My Drive/pesi_SSD/ssd300_Modanet_epoch-08_loss-6.7999_val_loss-6.8262.h5\n",
            "Epoch 9/120\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.001.\n",
            "256/256 [==============================] - 404s 2s/step - loss: 6.7047 - acc: 0.4635 - val_loss: 6.7396 - val_acc: 0.4630\n",
            "\n",
            "Epoch 00009: val_loss improved from 6.82616 to 6.73964, saving model to /content/drive/My Drive/pesi_SSD/ssd300_Modanet_epoch-09_loss-6.7047_val_loss-6.7396.h5\n",
            "Epoch 10/120\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.001.\n",
            "256/256 [==============================] - 403s 2s/step - loss: 6.6292 - acc: 0.4650 - val_loss: 6.6690 - val_acc: 0.4753\n",
            "\n",
            "Epoch 00010: val_loss improved from 6.73964 to 6.66896, saving model to /content/drive/My Drive/pesi_SSD/ssd300_Modanet_epoch-10_loss-6.6292_val_loss-6.6690.h5\n",
            "Epoch 11/120\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.001.\n",
            "256/256 [==============================] - 409s 2s/step - loss: 6.5369 - acc: 0.4671 - val_loss: 6.6647 - val_acc: 0.4956\n",
            "\n",
            "Epoch 00011: val_loss improved from 6.66896 to 6.66467, saving model to /content/drive/My Drive/pesi_SSD/ssd300_Modanet_epoch-11_loss-6.5358_val_loss-6.6647.h5\n",
            "Epoch 12/120\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.001.\n",
            "256/256 [==============================] - 401s 2s/step - loss: 6.4820 - acc: 0.4687 - val_loss: 6.5692 - val_acc: 0.4568\n",
            "\n",
            "Epoch 00012: val_loss improved from 6.66467 to 6.56916, saving model to /content/drive/My Drive/pesi_SSD/ssd300_Modanet_epoch-12_loss-6.4820_val_loss-6.5692.h5\n",
            "Epoch 13/120\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.001.\n",
            "256/256 [==============================] - 404s 2s/step - loss: 6.4383 - acc: 0.4675 - val_loss: 6.5119 - val_acc: 0.4953\n",
            "\n",
            "Epoch 00013: val_loss improved from 6.56916 to 6.51193, saving model to /content/drive/My Drive/pesi_SSD/ssd300_Modanet_epoch-13_loss-6.4402_val_loss-6.5119.h5\n",
            "Epoch 14/120\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.001.\n",
            "256/256 [==============================] - 403s 2s/step - loss: 6.3399 - acc: 0.4730 - val_loss: 6.4608 - val_acc: 0.4746\n",
            "\n",
            "Epoch 00014: val_loss improved from 6.51193 to 6.46082, saving model to /content/drive/My Drive/pesi_SSD/ssd300_Modanet_epoch-14_loss-6.3399_val_loss-6.4608.h5\n",
            "Epoch 15/120\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.001.\n",
            "256/256 [==============================] - 407s 2s/step - loss: 6.3154 - acc: 0.4704 - val_loss: 6.4353 - val_acc: 0.4760\n",
            "\n",
            "Epoch 00015: val_loss improved from 6.46082 to 6.43531, saving model to /content/drive/My Drive/pesi_SSD/ssd300_Modanet_epoch-15_loss-6.3154_val_loss-6.4353.h5\n",
            "Epoch 16/120\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.001.\n",
            "256/256 [==============================] - 402s 2s/step - loss: 6.2229 - acc: 0.4720 - val_loss: 6.3729 - val_acc: 0.4841\n",
            "\n",
            "Epoch 00016: val_loss improved from 6.43531 to 6.37285, saving model to /content/drive/My Drive/pesi_SSD/ssd300_Modanet_epoch-16_loss-6.2228_val_loss-6.3729.h5\n",
            "Epoch 17/120\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.001.\n",
            "256/256 [==============================] - 406s 2s/step - loss: 6.1950 - acc: 0.4747 - val_loss: 6.3443 - val_acc: 0.4877\n",
            "\n",
            "Epoch 00017: val_loss improved from 6.37285 to 6.34431, saving model to /content/drive/My Drive/pesi_SSD/ssd300_Modanet_epoch-17_loss-6.1950_val_loss-6.3443.h5\n",
            "Epoch 18/120\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.001.\n",
            "256/256 [==============================] - 400s 2s/step - loss: 6.1675 - acc: 0.4789 - val_loss: 6.3111 - val_acc: 0.4742\n",
            "\n",
            "Epoch 00018: val_loss improved from 6.34431 to 6.31111, saving model to /content/drive/My Drive/pesi_SSD/ssd300_Modanet_epoch-18_loss-6.1684_val_loss-6.3111.h5\n",
            "Epoch 19/120\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.001.\n",
            "256/256 [==============================] - 399s 2s/step - loss: 6.0649 - acc: 0.4757 - val_loss: 6.2780 - val_acc: 0.4806\n",
            "\n",
            "Epoch 00019: val_loss improved from 6.31111 to 6.27796, saving model to /content/drive/My Drive/pesi_SSD/ssd300_Modanet_epoch-19_loss-6.0649_val_loss-6.2780.h5\n",
            "Epoch 20/120\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.001.\n",
            "256/256 [==============================] - 407s 2s/step - loss: 6.0591 - acc: 0.4791 - val_loss: 6.2548 - val_acc: 0.4868\n",
            "\n",
            "Epoch 00020: val_loss improved from 6.27796 to 6.25476, saving model to /content/drive/My Drive/pesi_SSD/ssd300_Modanet_epoch-20_loss-6.0592_val_loss-6.2548.h5\n",
            "Epoch 21/120\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.001.\n",
            "256/256 [==============================] - 402s 2s/step - loss: 6.0050 - acc: 0.4781 - val_loss: 6.2181 - val_acc: 0.4649\n",
            "\n",
            "Epoch 00021: val_loss improved from 6.25476 to 6.21815, saving model to /content/drive/My Drive/pesi_SSD/ssd300_Modanet_epoch-21_loss-6.0059_val_loss-6.2181.h5\n",
            "Epoch 22/120\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.001.\n",
            "256/256 [==============================] - 399s 2s/step - loss: 5.9475 - acc: 0.4783 - val_loss: 6.1869 - val_acc: 0.4756\n",
            "\n",
            "Epoch 00022: val_loss improved from 6.21815 to 6.18689, saving model to /content/drive/My Drive/pesi_SSD/ssd300_Modanet_epoch-22_loss-5.9475_val_loss-6.1869.h5\n",
            "Epoch 23/120\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.001.\n",
            "256/256 [==============================] - 402s 2s/step - loss: 5.9301 - acc: 0.4821 - val_loss: 6.1987 - val_acc: 0.4799\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 6.18689\n",
            "Epoch 24/120\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.001.\n",
            "256/256 [==============================] - 399s 2s/step - loss: 5.8467 - acc: 0.4789 - val_loss: 6.1431 - val_acc: 0.4663\n",
            "\n",
            "Epoch 00024: val_loss improved from 6.18689 to 6.14308, saving model to /content/drive/My Drive/pesi_SSD/ssd300_Modanet_epoch-24_loss-5.8467_val_loss-6.1431.h5\n",
            "Epoch 25/120\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.001.\n",
            "256/256 [==============================] - 395s 2s/step - loss: 5.8261 - acc: 0.4769 - val_loss: 6.1208 - val_acc: 0.4814\n",
            "\n",
            "Epoch 00025: val_loss improved from 6.14308 to 6.12080, saving model to /content/drive/My Drive/pesi_SSD/ssd300_Modanet_epoch-25_loss-5.8261_val_loss-6.1208.h5\n",
            "Epoch 26/120\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.001.\n",
            "251/256 [============================>.] - ETA: 6s - loss: 5.7913 - acc: 0.4863"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrSixhhJUpz0",
        "colab_type": "text"
      },
      "source": [
        "# Save weight on drive  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jziGuwcg36dN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once in a notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Create & upload file.\n",
        "uploaded = drive.CreateFile({'title': 'ssd300_Modanet_data-aug_epoch-30_loss-6.4659_val_loss-6.1568.h5'})\n",
        "uploaded.SetContentFile('/content/ssd300_Modanet_data-aug_epoch-30_loss-6.4659_val_loss-6.1568.h5') #aggiungere path file\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDkOwQ996HcH",
        "colab_type": "text"
      },
      "source": [
        "# Calcolo dell' IoU \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVVHN2HbBWKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_generator = val_dataset.generate(batch_size=1,\n",
        "                                         shuffle=True,\n",
        "                                         transformations=[convert_to_3_channels,\n",
        "                                                          resize],\n",
        "                                         label_encoder=None,\n",
        "                                         returns={'processed_images',\n",
        "                                                  'filenames',\n",
        "                                                  'inverse_transform',\n",
        "                                                  'original_images',\n",
        "                                                  'original_labels'},\n",
        "                                         keep_images_without_gt=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lV-JGLApGvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
        "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
        "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
        "from statistics import mean\n",
        "classes = ['background',\n",
        "           'bag', 'belt', 'boots', 'footwear',\n",
        "           'outer', 'dress', 'sunglasses', 'pants',\n",
        "           'top', 'shorts', 'skirt', 'headwear',\n",
        "           'scarf/tie']\n",
        "def iou_metric(y_true, y_pred):\n",
        "    y_pred_decoded = decode_detections(y_pred,\n",
        "                                   confidence_thresh=0.5,\n",
        "                                   iou_threshold=0.4,\n",
        "                                   top_k=200,\n",
        "                                   normalize_coords=normalize_coords,\n",
        "                                   img_height=img_height,\n",
        "                                   img_width=img_width)\n",
        "    y_pred_decoded_inv = apply_inverse_transforms(y_pred_decoded, batch_inverse_transforms)\n",
        "\n",
        "    iou_list= []\n",
        "    AoG = 0\n",
        "    AoP = 0\n",
        "    xmin_gt=0 \n",
        "    ymin_gt=0\n",
        "    xmax_gt=0\n",
        "    ymax_gt = 0\n",
        "    xmin_pred=0\n",
        "    ymin_pred=0\n",
        "    xmax_pred=0\n",
        "    ymax_pred = 0\n",
        "\n",
        "    for i in range(0,len(batch_original_labels)):\n",
        "      for box in batch_original_labels[i]:\n",
        "        for box_pred in y_pred_decoded_inv[i]:\n",
        "          if (classes[int(box_pred[0])]==classes[int(box[0])]):\n",
        "            xmin_gt = box[1]\n",
        "            ymin_gt = box[2]\n",
        "            xmax_gt = box[3]\n",
        "            ymax_gt = box[4]\n",
        "            AoG = abs(xmax_gt-xmin_gt) * abs(ymax_gt-ymin_gt)\n",
        "            xmin_pred = box_pred[2]\n",
        "            ymin_pred = box_pred[3]\n",
        "            xmax_pred = box_pred[4]\n",
        "            ymax_pred = box_pred[5]\n",
        "            AoP = abs(xmax_pred-xmin_pred) * abs(ymax_pred-ymin_pred)\n",
        "            \n",
        "            # overlaps are the co-ordinates of intersection box\n",
        "            overlap_0 = max(xmin_gt, xmin_pred)\n",
        "            overlap_1 = min(xmax_gt, xmax_pred)\n",
        "            overlap_2 = max(ymin_gt, ymin_pred)\n",
        "            overlap_3 = min(ymax_gt, ymax_pred)\n",
        "\n",
        "            # intersection area\n",
        "            intersection = abs((overlap_3 - overlap_2) * (overlap_1 - overlap_0))\n",
        "            # area of union of both boxes\n",
        "            union = AoG + AoP - intersection\n",
        "\n",
        "            # iou calculation\n",
        "            temp=intersection/union\n",
        "            iou_list.append(temp)\n",
        "    #print(iou_list)\n",
        "    if not(iou_list ==[]):\n",
        "      iou=mean(iou_list)\n",
        "      return iou\n",
        "     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG4c9iYMCJbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iou_list=[]\n",
        "for i in range(0,val_dataset.dataset_size):\n",
        "  batch_images, batch_filenames, batch_inverse_transforms, batch_original_images, batch_original_labels = next(predict_generator)\n",
        "  y_pred = model.predict(batch_images)\n",
        "  iou=iou_metric(batch_original_labels,y_pred)\n",
        "  iou_list.append(iou)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv6M3SyOG_vy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(iou_list)\n",
        "lungh=len(iou_list)\n",
        "print(lungh)\n",
        "somma=0\n",
        "for i in iou_list:\n",
        "  if not i is None: \n",
        "    somma+=i\n",
        "print(\"L'IoU media Ã¨ \" + str(somma/lungh))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-EZc3zl6Uhz",
        "colab_type": "text"
      },
      "source": [
        "# Generazione dei bounding box predetti e reali "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0KdpEcZCOx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(batch_images)\n",
        "\n",
        "y_pred_decoded = decode_detections(y_pred,\n",
        "                                   confidence_thresh=0.5,\n",
        "                                   iou_threshold=0.4,\n",
        "                                   top_k=200,\n",
        "                                   normalize_coords=normalize_coords,\n",
        "                                   img_height=img_height,\n",
        "                                   img_width=img_width)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBGKvK2uCW0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_decoded_inv = apply_inverse_transforms(y_pred_decoded, batch_inverse_transforms)\n",
        "\n",
        "np.set_printoptions(precision=2, suppress=True, linewidth=90)\n",
        "print(\"Predicted boxes:\\n\")\n",
        "print('   class   conf xmin   ymin   xmax   ymax')\n",
        "print(y_pred_decoded_inv[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85bZwS-aCcE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "colors = plt.cm.hsv(np.linspace(0, 1, n_classes+1)).tolist()\n",
        "\n",
        "classes = ['background',\n",
        "           'bag', 'belt', 'boots', 'footwear',\n",
        "           'outer', 'dress', 'sunglasses', 'pants',\n",
        "           'top', 'shorts', 'skirt', 'headwear',\n",
        "           'scarf/tie']\n",
        "\n",
        "plt.figure(figsize=(20,12))\n",
        "plt.imshow(batch_original_images[i])\n",
        "\n",
        "current_axis = plt.gca()\n",
        "\n",
        "for box in batch_original_labels[i]:\n",
        "    xmin = box[1]\n",
        "    ymin = box[2]\n",
        "    xmax = box[3]\n",
        "    ymax = box[4]\n",
        "    label = '{}'.format(classes[int(box[0])])\n",
        "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color='green', fill=False, linewidth=2))  \n",
        "    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':'green', 'alpha':1.0})\n",
        "\n",
        "for box in y_pred_decoded_inv[i]:\n",
        "    xmin = box[2]\n",
        "    ymin = box[3]\n",
        "    xmax = box[4]\n",
        "    ymax = box[5]\n",
        "    color = colors[int(box[0])]\n",
        "    label = '{}: {:.2f}'.format(classes[int(box[0])], box[1])\n",
        "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))  \n",
        "    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':color, 'alpha':1.0})"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}